{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jkutpiOyYsC"
      },
      "source": [
        "# PCA by:\n",
        "\n",
        "* Daniela Martinez Quiroga\n",
        "* María Isabella Rodríguez Arévalo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptHYZq6eyee8"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE10OBFfz40P"
      },
      "outputs": [],
      "source": [
        "# Tensorfolw and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "#Ensemble\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#SVM\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "#Hyperparameters\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Utils for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#Metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvSr0xWkziUv"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fPB44U60YQR"
      },
      "source": [
        "## Loading and overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "IrUtdNB3zlNU",
        "outputId": "4f3ff167-4ce2-4bcd-8648-ffaadd4ddca0"
      },
      "outputs": [],
      "source": [
        "# 1. Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# 2. Training data dimensions\n",
        "print(\"Dimensiones de x_train:\", x_train.shape)\n",
        "print(\"Dimensiones de y_train:\", y_train.shape)\n",
        "\n",
        "# 2. See samples\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(y_train[i])  # Shows numeric label\n",
        "plt.show()\n",
        "\n",
        "# 3. Classes\n",
        "num_classes = len(np.unique(y_train))\n",
        "print(\"Número de clases:\", num_classes)\n",
        "\n",
        "# Classes' names\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "print(\"Nombres de las clases:\", class_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES62s_XK0ex2"
      },
      "source": [
        "## Flattering, normalization, missing values\n",
        "It is discovered that, fortunately, there are no null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbMqKrJG0fFi",
        "outputId": "463d27b4-99b1-4475-f005-a43d1ee2ed10"
      },
      "outputs": [],
      "source": [
        "# 1. Normalize pixel values:\n",
        "# Pixel values are originally in the range [0, 255]\n",
        "# Dividing by 255 scales them to the range [0, 1].\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# 2. Flattering\n",
        "# Converts the 28x28 images into 784-dimensional vectors.\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "# Are there any nulls (NaN) ?\n",
        "print(\"¿Hay valores NaN en x_train?\", np.isnan(x_train).any())\n",
        "print(\"¿Hay valores NaN en x_test?\", np.isnan(x_test).any())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NonpqYJX2htT"
      },
      "source": [
        "# PCA and Components\n",
        "When analyzing the cumulative explained variance, a relatively small number of principal components is required to retain a high percentage of the total variance. Initially, 784 components were established, but as can be seen in the results:\n",
        "\n",
        "* For 85% variance: 43 principal components.\n",
        "* For 90% variance: 84 principal components.\n",
        "* For 95% variance: 188 principal components.\n",
        "\n",
        "This means that the dimensionality of the dataset can be reduced without losing much information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "Upf2FXCs2jg8",
        "outputId": "05e6138e-9ecc-43a3-d4e8-72c3e817ad15"
      },
      "outputs": [],
      "source": [
        "# 1. Instantiate and fit PCA model\n",
        "pca = PCA(n_components=784)\n",
        "pca.fit(x_train)\n",
        "\n",
        "# 2. Explained variance\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Graph of the variance explained by each component\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, 785), explained_variance_ratio, marker='o', linestyle='--')\n",
        "plt.title('Varianza Explicada por Componente Principal')\n",
        "plt.xlabel('Componente Principal')\n",
        "plt.ylabel('Varianza Explicada')\n",
        "plt.show()\n",
        "\n",
        "# Graph of the cumulative explained variance\n",
        "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, 785), cumulative_variance_ratio, marker='o', linestyle='--')\n",
        "plt.title('Varianza Explicada Acumulada')\n",
        "plt.xlabel('Número de Componentes Principales')\n",
        "plt.ylabel('Varianza Explicada Acumulada')\n",
        "plt.show()\n",
        "\n",
        "# 3. Selección del número de componentes\n",
        "for desired_variance in [0.85, 0.90, 0.95]:\n",
        "    n_components = np.argmax(cumulative_variance_ratio >= desired_variance) + 1\n",
        "    print(f\"Para retener el {desired_variance*100:.0f}% de la varianza, se necesitan {n_components} componentes principales.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfPfTz9tyXNg"
      },
      "source": [
        "# Visualization and Analysis about Principals components\n",
        "\n",
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "L8cd-PIXywW1",
        "outputId": "b802588e-40e2-4fb3-9380-822b05131877"
      },
      "outputs": [],
      "source": [
        "# Principal components are in components_\n",
        "components = pca.components_\n",
        "\n",
        "# Reshape firsts principal components to the original shape\n",
        "data_reshape = components[:15].reshape(15,28,28)\n",
        "\n",
        "# Showing these components\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(15):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(data_reshape[i], cmap=plt.cm.binary)\n",
        "\n",
        "plt.suptitle('Firts 15 principal components')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exOHxc-m5HBy"
      },
      "source": [
        "### Analysis\n",
        "\n",
        "These pictures are 15 \"eigen-fashions\" pictures after making PCA. We can see in each picture has a mix among diferents clothes. For example, in the first one are a shoe and a blouse; in the second one are pants and a blouse; etc. Also in last pictures it's difficult to identify all the clothes. To sum up, each picture are representing different clothes, however in some cases it's hard to recognize the elements in the picture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsnFho0Z7I7v"
      },
      "source": [
        "## Data projection and 2D Visualization\n",
        "\n",
        "In this step, the train data will be transformed in a PCA process with components=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "i7Pkcl0G0tee",
        "outputId": "02435fe0-eeaf-46a1-ffd0-76a2fb004455"
      },
      "outputs": [],
      "source": [
        "# Creating PCA\n",
        "pca_2 = PCA(n_components=2)\n",
        "pca_2.fit(x_train)\n",
        "\n",
        "# Transforming the train data\n",
        "x_train_2d = pca_2.transform(x_train)\n",
        "\n",
        "#Scatter Plot\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(num_classes):\n",
        "  idx = y_train == i\n",
        "  plt.scatter(x_train_2d[idx, 0], x_train_2d[idx, 1],\n",
        "              label=class_names[i],color=plt.cm.tab10(i),alpha=0.6)\n",
        "plt.xlabel(\"Principal component 1\")\n",
        "plt.ylabel(\"Principal component 2\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2tlxU9LCtIm"
      },
      "source": [
        "### Analysis\n",
        "\n",
        "In the plot, it is noticed that the components are superimposed. Despite that, there is possible to identify which components are closer than others such as ankle boot or dress. Trouser, sandal and t-shirt are difficult to evalute how closer or dispersed are the data. For that reason, it has to be considered to look for more components in PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6RXgyruMWQ1"
      },
      "source": [
        "# Using PCA for modeling\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPDzGsT0NIdU"
      },
      "outputs": [],
      "source": [
        "# PCA models, taking the calculated components before\n",
        "pca_85 = PCA(n_components=43)\n",
        "pca_85.fit(x_train)\n",
        "pca_90 = PCA(n_components=84)\n",
        "pca_90.fit(x_train)\n",
        "pca_95 = PCA(n_components=188)\n",
        "pca_95.fit(x_train)\n",
        "\n",
        "# Transforming data\n",
        "x_train_85 = pca_85.transform(x_train)\n",
        "x_test_85 = pca_85.transform(x_test)\n",
        "\n",
        "x_train_90 = pca_90.transform(x_train)\n",
        "x_test_90 = pca_90.transform(x_test)\n",
        "\n",
        "x_train_95 = pca_95.transform(x_train)\n",
        "x_test_95 = pca_95.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9DoImT_QlBS"
      },
      "source": [
        "## SVM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "oUa3Zv9CQrL2",
        "outputId": "aceff1ef-5372-45e1-c4a4-cad19be7d06e"
      },
      "outputs": [],
      "source": [
        "svm_85 = SVC(kernel='rbf', gamma='auto', max_iter=1000)\n",
        "svm_90 = SVC(kernel='rbf', gamma='auto', max_iter=1000)\n",
        "svm_95 = SVC(kernel='rbf', gamma='auto', max_iter=1000)\n",
        "\n",
        "svm_85.fit(x_train_85, y_train)\n",
        "svm_90.fit(x_train_90, y_train)\n",
        "svm_95.fit(x_train_95, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apVFWzVNUqvM"
      },
      "outputs": [],
      "source": [
        "y_beanie85_t = svm_85.predict(x_test_85)\n",
        "y_beanie85_tr = svm_85.predict(x_train_85)\n",
        "y_beanie90_t = svm_90.predict(x_test_90)\n",
        "y_beanie90_tr = svm_90.predict(x_train_90)\n",
        "y_beanie95_t = svm_95.predict(x_test_95)\n",
        "y_beanie95_tr = svm_95.predict(x_train_95)\n",
        "\n",
        "accuracy85_t = accuracy_score(y_test, y_beanie85_t)\n",
        "accuracy85_tr = accuracy_score(y_train, y_beanie85_tr)\n",
        "accuracy90_t = accuracy_score(y_test, y_beanie90_t)\n",
        "accuracy90_tr = accuracy_score(y_train, y_beanie90_tr)\n",
        "accuracy95_t = accuracy_score(y_test, y_beanie95_t)\n",
        "accuracy95_tr = accuracy_score(y_train, y_beanie95_tr)\n",
        "\n",
        "f1_score_85_t = f1_score(y_test, y_beanie85_t, average='macro')\n",
        "f1_score_85_tr = f1_score(y_train, y_beanie85_tr, average='macro')\n",
        "f1_score_90_t = f1_score(y_test, y_beanie90_t, average='macro')\n",
        "f1_score_90_tr = f1_score(y_train, y_beanie90_tr, average='macro')\n",
        "f1_score_95_t = f1_score(y_test, y_beanie95_t, average='macro')\n",
        "f1_score_95_tr = f1_score(y_train, y_beanie95_tr, average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRtj7R1wbjlq"
      },
      "source": [
        "# RandomizedForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "R6HoEQ5TbpBs",
        "outputId": "03fe4292-d4fe-411b-8d3e-e384e58fd50e"
      },
      "outputs": [],
      "source": [
        "rf_85 = RandomForestClassifier(\n",
        "    n_estimators=50, max_depth=20,\n",
        "    min_samples_split=5, min_samples_leaf=2,\n",
        "    max_features='sqrt', random_state=42\n",
        ")\n",
        "\n",
        "rf_90 = RandomForestClassifier(\n",
        "    n_estimators=50, max_depth=20,\n",
        "    min_samples_split=5, min_samples_leaf=2,\n",
        "    max_features='sqrt', random_state=42\n",
        ")\n",
        "\n",
        "rf_95 = RandomForestClassifier(\n",
        "    n_estimators=50, max_depth=20,\n",
        "    min_samples_split=5, min_samples_leaf=2,\n",
        "    max_features='sqrt', random_state=42\n",
        ")\n",
        "\n",
        "rf_85.fit(x_train_85, y_train)\n",
        "rf_90.fit(x_train_90, y_train)\n",
        "rf_95.fit(x_train_95, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj66iYPKdfSd"
      },
      "outputs": [],
      "source": [
        "y_beanie85_rf_t = rf_85.predict(x_test_85)\n",
        "y_beanie85_rf_tr = rf_85.predict(x_train_85)\n",
        "y_beanie90_rf_t = rf_90.predict(x_test_90)\n",
        "y_beanie90_rf_tr = rf_90.predict(x_train_90)\n",
        "y_beanie95_rf_t = rf_95.predict(x_test_95)\n",
        "y_beanie95_rf_tr = rf_95.predict(x_train_95)\n",
        "\n",
        "accuracy85_rf_t = accuracy_score(y_test, y_beanie85_rf_t)\n",
        "accuracy85_rf_tr = accuracy_score(y_train, y_beanie85_rf_tr)\n",
        "accuracy90_rf_t = accuracy_score(y_test, y_beanie90_rf_t)\n",
        "accuracy90_rf_tr = accuracy_score(y_train, y_beanie90_rf_tr)\n",
        "accuracy95_rf_t = accuracy_score(y_test, y_beanie95_rf_t)\n",
        "accuracy95_rf_tr = accuracy_score(y_train, y_beanie95_rf_tr)\n",
        "\n",
        "f1_score_rf_85_t = f1_score(y_test, y_beanie85_rf_t, average='macro')\n",
        "f1_score_rf_85_tr = f1_score(y_train, y_beanie85_rf_tr, average='macro')\n",
        "f1_score_rf_90_t = f1_score(y_test, y_beanie90_rf_t, average='macro')\n",
        "f1_score_rf_90_tr = f1_score(y_train, y_beanie90_rf_tr, average='macro')\n",
        "f1_score_rf_95_t = f1_score(y_test, y_beanie95_rf_t, average='macro')\n",
        "f1_score_rf_95_tr = f1_score(y_train, y_beanie95_rf_tr, average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1p9nWYoerey"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tv5QBpJeu2b",
        "outputId": "70f3cf21-f039-4b57-f063-6094d59311c4"
      },
      "outputs": [],
      "source": [
        "model_85 = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(43,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_85.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_90 = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(84,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_90.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_95 = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(188,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_95.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CfXSrl9fnvH",
        "outputId": "eac97ebe-2da0-47b6-8047-b386e354e284"
      },
      "outputs": [],
      "source": [
        "model_85.fit(x_train_85, y_train, epochs=10, validation_split=0.1)\n",
        "model_90.fit(x_train_90, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "model_95.fit(x_train_95, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "y_beanie85_rn_t = model_85.predict(x_test_85)\n",
        "y_beanie85_rn_tr = model_85.predict(x_train_85)\n",
        "y_beanie90_rn_t = model_90.predict(x_test_90)\n",
        "y_beanie90_rn_tr = model_90.predict(x_train_90)\n",
        "y_beanie95_rn_t = model_95.predict(x_test_95)\n",
        "y_beanie95_rn_tr = model_95.predict(x_train_95)\n",
        "\n",
        "loss85_t, accuracy85_rn_t = model_85.evaluate(x_test_85, y_test)\n",
        "loss90_t, accuracy90_rn_t = model_90.evaluate(x_test_90, y_test)\n",
        "loss95_t, accuracy95_rn_t = model_95.evaluate(x_test_95, y_test)\n",
        "\n",
        "loss85_tr, accuracy85_rn_tr = model_85.evaluate(x_train_85, y_train)\n",
        "loss90_tr, accuracy90_rn_tr = model_90.evaluate(x_train_90, y_train)\n",
        "loss95_tr, accuracy95_rn_tr = model_95.evaluate(x_train_95, y_train)\n",
        "\n",
        "f1_score_rn_85_t = f1_score(y_test, y_beanie85_rn_t.argmax(axis=1), average='macro')\n",
        "f1_score_rn_85_tr = f1_score(y_train, y_beanie85_rn_tr.argmax(axis=1), average='macro')\n",
        "f1_score_rn_90_t = f1_score(y_test, y_beanie90_rn_t.argmax(axis=1), average='macro')\n",
        "f1_score_rn_90_tr = f1_score(y_train, y_beanie90_rn_tr.argmax(axis=1), average='macro')\n",
        "f1_score_rn_95_t = f1_score(y_test, y_beanie95_rn_t.argmax(axis=1), average='macro')\n",
        "f1_score_rn_95_tr = f1_score(y_train, y_beanie95_rn_tr.argmax(axis=1), average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2jl9yEFjXz4"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaeCek_4jaOX",
        "outputId": "c3e80a4d-2287-48c2-d383-1e907c46338a"
      },
      "outputs": [],
      "source": [
        "table_tr = {\n",
        "    \"Model\": [\"SVM\", \"Random Forest\", \"Red Neuronal\"],\n",
        "    \"Accuracy 85\": [accuracy85_tr, accuracy85_rf_tr, accuracy85_rn_tr],\n",
        "    \"Accuracy 90\": [accuracy90_tr, accuracy90_rf_tr, accuracy90_rn_tr],\n",
        "    \"Accuracy 95\": [accuracy95_tr, accuracy95_rf_tr, accuracy95_rn_tr],\n",
        "    \"F1 Score 85\": [f1_score_85_tr, f1_score_rf_85_tr, f1_score_rn_85_tr],\n",
        "    \"F1 Score 90\": [f1_score_90_tr, f1_score_rf_90_tr, f1_score_rn_90_tr],\n",
        "    \"F1 Score 95\": [f1_score_95_tr, f1_score_rf_95_tr, f1_score_rn_95_tr]\n",
        "}\n",
        "\n",
        "table_t = {\n",
        "    \"Model\": [\"SVM\", \"Random Forest\", \"Red Neuronal\"],\n",
        "    \"Accuracy 85\": [accuracy85_t, accuracy85_rf_t, accuracy85_rn_t],\n",
        "    \"Accuracy 90\": [accuracy90_t, accuracy90_rf_t, accuracy90_rn_t],\n",
        "    \"Accuracy 95\": [accuracy95_t, accuracy95_rf_t, accuracy95_rn_t],\n",
        "    \"F1 Score 85\": [f1_score_85_t, f1_score_rf_85_t, f1_score_rn_85_t],\n",
        "    \"F1 Score 90\": [f1_score_90_t, f1_score_rf_90_t, f1_score_rn_95_t],\n",
        "    \"F1 Score 95\": [f1_score_95_t, f1_score_rf_95_t, f1_score_rn_95_t]\n",
        "}\n",
        "\n",
        "df_tabla_tr = pd.DataFrame(table_tr)\n",
        "df_tabla_t = pd.DataFrame(table_t)\n",
        "print(df_tabla_tr)\n",
        "print(df_tabla_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8HWQCY4msRx"
      },
      "source": [
        "1. Looking into the final results, the best model is the neural network according with the train data and test data. However, the results in test data in all the models got results under 0.9 which isn't perfect or ideal for a model.\n",
        "2. **TEST DATA SCORES:** The neural network has the best results. Also, this was improving in each variance results showing us this could work better with a high variance. On the other hand, SVM and Random forest decrease the accuracy and f1 score results when the variance was high.\n",
        "3. **TRAIN DATA SCORES:** The random forest has the best results and the scores increse when the variance was high. The same situation happens with neural network but this doesn't have the highest score. SVM has the contrary situation, its scores decrease with high variance.\n",
        "4. We concluded that SVM couldn't work with high variance in its datasets beacuase the accuracy and f1 score got bad numbers. Second, Random Forest depends in other hyperparameters or parameters to increase or decrease its scores beacuse the variances doesn't have the same influence in the data. Finally, neural network has a postitve relation with the variance since it has shown its scores improve with more variance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
